---
title: "Quickstart - Pt 2"
icon: "square-2"
---

<Info>This continues from [Quickstart Part 1](/quickstart), where we built a cake recipe generator prompt.</Info>

In Part 1, you created a prompt, tested it in the playground, and learned about versioning. Now let's evaluate prompt quality, test different models, and connect everything to your code.

## Evaluating a Prompt

Before deploying a prompt, you want to know if it's actually good. PromptLayer lets you build evaluation pipelines that score your prompt's outputs automatically.

### Creating a Dataset

Evaluations run against a dataset - a collection of test cases with inputs and expected outputs. Let's create one for our cake recipe prompt.

Click **New** → **Dataset** and name it "cake-recipes-test".

<Frame>
  <img src="https://placehold.co/1200x600/e8e8e8/999999?text=Screenshot+needed:+Creating+dataset" alt="Creating a dataset" />
</Frame>

Add a few test cases. Each row needs the input variables your prompt expects (`cake_type`, `serving_size`) and optionally an expected output to compare against:

<Accordion title="Sample CSV for cake recipe dataset">
```csv
cake_type,serving_size,expected_output
Chocolate Cake,8,"Should include cocoa or chocolate, have clear measurements"
Vanilla Birthday Cake,12,"Should be festive, mention frosting options"
Gluten-Free Lemon Cake,6,"Must not include wheat flour, should use alternatives"
Vegan Carrot Cake,10,"No eggs or dairy, should suggest substitutes"
```

You can upload this CSV directly or add rows manually in the UI.
</Accordion>

Learn more about [Datasets](/features/evaluations/datasets).

### Creating an Eval Pipeline

Now let's build a pipeline that runs your prompt against each test case and scores the results.

Click **New** → **Evaluation** and select your dataset.

<Frame>
  <img src="https://placehold.co/1200x600/e8e8e8/999999?text=Screenshot+needed:+Eval+pipeline+setup" alt="Eval pipeline setup" />
</Frame>

Add an **LLM-as-judge** scoring column. This uses AI to score each output against criteria you define. For our recipe prompt, we might check:

- Does the recipe include all required sections (Overview, Ingredients, Instructions)?
- Are measurements provided in both metric and US units?
- Is the serving size correct?

<Frame>
  <img src="https://placehold.co/1200x600/e8e8e8/999999?text=Screenshot+needed:+LLM+as+judge+configuration" alt="LLM as judge" />
</Frame>

<Accordion title="Other evaluation types">
  Beyond LLM-as-judge, PromptLayer supports:
  
  - **Human grading**: Collect scores from domain experts
  - **Diff comparison**: Compare outputs character-by-character to expected results
  - **Cosine similarity**: Measure semantic similarity between outputs
  - **Code evaluators**: Write custom Python scoring functions
  
  Agent nodes work the same way in eval pipelines.
</Accordion>

Run the evaluation to see scores across all test cases. Learn more about [Evaluations](/features/evaluations/overview).

### Testing Different Models

Want to see how your prompt performs on Claude vs GPT-4 vs Gemini? Add multiple columns to your eval pipeline, each with a different model override.

In your pipeline, click **Add Column** and configure each one to use a different model. The pipeline runs your prompt on each model and shows results side by side.

<Frame>
  <img src="https://placehold.co/1200x600/e8e8e8/999999?text=Screenshot+needed:+Model+comparison+columns" alt="Comparing models" />
</Frame>

This helps you find the best price/performance balance for your use case. Learn more about [supported providers](/features/supported-providers).

## Historical Backtests

Once your prompt is in production, you'll have real request logs. Use these to test new prompt versions against actual user inputs.

### Creating a Historical Dataset

Go to **Datasets** and click **Add from Request History**. This opens a request log browser where you can filter and select requests.

<Frame>
  <img src="https://placehold.co/1200x600/e8e8e8/999999?text=Screenshot+needed:+Add+from+request+history" alt="Adding from request history" />
</Frame>

Filter by prompt name, date range, metadata, or search content. Select the requests you want and click **Add Requests**. This captures the real inputs your users sent, along with the outputs your current prompt produced.

### Running a Backtest

Create an evaluation that runs your new prompt version against this historical dataset. Add columns for:

- **New prompt output**: The response from your updated prompt version
- **Diff**: Side-by-side comparison highlighting changes from the original output

<Frame>
  <img src="https://placehold.co/1200x600/e8e8e8/999999?text=Screenshot+needed:+Backtest+results" alt="Backtest results" />
</Frame>

Backtests are powerful because they show impact on real user data without needing to define exact pass/fail criteria upfront. You can quickly spot if a change produces dramatically different outputs.

Learn more about [backtesting](/features/evaluations/continuous-integration#backtesting).

## CI/CD Evaluation

Attach an evaluation pipeline to run automatically every time you save a new prompt version - similar to GitHub Actions running tests on each commit.

When saving a prompt, the commit dialog lets you select an evaluation pipeline. Choose one and click **Next**.

From then on, each new version you create will run through the eval and show its score in the version history. This makes it easy to spot regressions before they reach production.

<Frame>
  <img src="https://placehold.co/1200x600/e8e8e8/999999?text=Screenshot+needed:+Version+scores+percentage" alt="Eval scores by version" />
</Frame>

Learn more about [continuous integration](/features/evaluations/continuous-integration).

## Connecting in Code

PromptLayer serves as the source of truth for your prompts. Your application fetches prompts by name, keeping prompt logic out of your codebase and enabling non-engineers to make changes.

### Installation

<CodeGroup>

```bash Python
pip install promptlayer
```

```bash JavaScript
npm install promptlayer
```

</CodeGroup>

### Running Prompts

Initialize the client and run a prompt. The SDK fetches the prompt template from PromptLayer, runs it against your configured LLM provider locally, then logs the result back.

<CodeGroup>

```python Python
from promptlayer import PromptLayer

client = PromptLayer()  # Uses PROMPTLAYER_API_KEY env var

response = client.run(
    prompt_name="cake-recipe",
    input_variables={
        "cake_type": "Chocolate Cake",
        "serving_size": "8"
    }
)
```

```javascript JavaScript
import { PromptLayer } from "promptlayer";

const client = new PromptLayer();

const response = await client.run({
    promptName: "cake-recipe",
    inputVariables: {
        cake_type: "Chocolate Cake",
        serving_size: "8"
    }
});
```

</CodeGroup>

<Accordion title="Output Format">
  The response includes a [prompt blueprint](/running-requests/prompt-blueprints) - a model-agnostic format that works the same whether you're using OpenAI, Anthropic, or any other provider. Access the generated content with:
  
  ```python
  response["prompt_blueprint"]["prompt_template"]["messages"][-1]["content"]
  ```
  
  This lets you switch models without changing how you read responses.
</Accordion>

Use `prompt_release_label="production"` to fetch the version labeled for production. Use `prompt_version=3` to pin to a specific version number. Agents work the same way - just pass the agent name.

<Accordion title="Environment variables">
Store your API keys as environment variables:

```bash
export PROMPTLAYER_API_KEY="your-api-key"
export OPENAI_API_KEY="your-openai-key"
```

The client reads these automatically.
</Accordion>

### Metadata and Logging

Add metadata to track requests by user, session, or feature flag:

```python
response = client.run(
    prompt_name="cake-recipe",
    input_variables={"cake_type": "Chocolate", "serving_size": "8"},
    tags=["production", "recipe-feature"],
    metadata={
        "user_id": "user_123",
        "session_id": "sess_abc"
    }
)

# Add a score after reviewing the output
client.track.score(request_id=response["request_id"], score=95)
```

This metadata appears in your logs, letting you filter and debug by user or feature. Learn more about [metadata](/features/prompt-history/metadata) and [tagging](/features/prompt-history/tagging-requests).

For agents or complex workflows, enable tracing with `client = PromptLayer(enable_tracing=True)` and use the `@client.traceable` decorator on your functions to see each step as spans. Learn more about [traces](/running-requests/traces).

## Organizations

Use [organizations and workspaces](/why-promptlayer/shared-workspaces) to manage teams and environments. Common setups include separate workspaces for Production, Staging, and Development.

<Frame>
  <img src="https://placehold.co/1200x600/e8e8e8/999999?text=Screenshot+needed:+Changelog" alt="Changelog" />
</Frame>

Key features:
- **Role-based access control**: Owner, Admin, Editor, and Viewer roles at organization and workspace levels
- **Audit logs**: Track who changed what and when
- **Author attribution**: See who created and modified each prompt version
- **Centralized billing**: Manage usage across all workspaces

Each workspace has its own API key. Switch between workspaces using the dropdown in the top navigation.

## Next Steps

- [Deployment Strategies](/onboarding-guides/deployment-strategies) - Caching, webhooks, and production patterns
- [Tutorial Videos](/tutorial-videos) - Watch walkthroughs of common workflows
- [API Reference](/reference) - Full SDK documentation
