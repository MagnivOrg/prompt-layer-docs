---
title: "Quickstart Part 2"
icon: "rocket"
---

Take your PromptLayer skills to the next level with advanced features designed for data-driven evaluations and fine-tuning workflows.

## Key Features
- **Create Datasets**: Use CSV/JSON uploads or historical data to build datasets
- **Eval Pipelines**: Create evaluation pipelines for comprehensive prompt assessments
- **Backtesting**: Test new prompt versions against historical data to detect regressions and validate improvements
- **Analytics**: Leverage detailed metrics to analyze and improve prompt performance
- **Fine-Tuning**: Build specialized models with your existing data
- **Tracing**: Gain insights into your application's execution flow
- **Custom Evaluations**: Use the visual pipeline builder to set up custom scoring criteria and evaluation steps
- **Workspaces**: Collaborate with your team and manage projects seamlessly

## Creating Datasets

### CSV/JSON Upload
Navigate to the Datasets section from the dashboard and click "Upload Dataset". Choose a CSV or JSON file containing your data, then follow the prompts to map and validate your data fields.

<Tip>
  This method is ideal for incorporating pre-existing data for evaluations or batch runs.
</Tip>

### Historical Data
In the Datasets section, click "Create from Historical Data". Apply filters to refine your dataset:

- **Time Range**: Specify start and end times for included requests
- **Prompt Templates**: Filter by specific template names and versions
- **Metadata**: Include requests with matching key-value pairs
- **Tags**: Select requests based on their assigned tags
- **Scores**: Use scoring criteria, such as user feedback
- **Search Query**: Enter query strings for granular filtering

<Info>
  Historical datasets are perfect for backtesting and continuous regression testing with real-world production data.
</Info>

## Eval Pipeline Creation

### Select Your Dataset
- Choose an existing dataset or create a new one via upload or historical data
- Use filtering options to tailor the dataset to your needs

### Build Your Pipeline
- Create a new batch run and give it a name
- Use the visual pipeline builder to define each step, from data processing to evaluation

### Preview and Run
- Use preview mode to see live feedback and adjust the pipeline in real time
- Execute your pipeline and analyze the results in a spreadsheet-like interface

<Tip>
  Evaluation pipelines allow you to backtest new prompt versions, compare outputs, and ensure consistent performance over time.
</Tip>

## Backtesting

Backtesting is a powerful tool for evaluating new prompt versions against historical data:

### Process
1. **Create a Historical Dataset**
   - Use your logged requests
   - Apply filters to select relevant data

2. **Build an Evaluation Pipeline**
   - Create a batch run
   - Select your dataset
   - Configure the pipeline to compare outputs

3. **Run the Backtest**
   - Execute the pipeline
   - Generate comparisons with diff view
   - Analyze side-by-side results

### Benefits
- Detect regressions in prompt behavior
- Validate improvements before deploying new versions
- Ensure stability across various scenarios

## Analytics

PromptLayer provides robust analytics to help optimize your prompts:

- **Backtesting**: Validate new versions using historical data
- **Evaluation Pipelines**: Use metrics to identify areas for improvement
- **Datasets**: Analyze trends and insights from CSV/JSON uploads or historical data
- **Batch Jobs**: Experiment with prompts at scale for iteration and improvement
- **Continuous Integration**: Automatically evaluate new prompt versions to maintain quality

## Fine-Tuning

Fine-tuning allows you to create specialized models tailored to your needs:

### Setup Process
1. **Data Collection**: Use logged GPT-4 requests as training data
2. **Model Selection**: Fine-tune a smaller model (e.g., GPT-3.5-turbo) to replicate GPT-4 quality
3. **Easy Setup**: Initiate fine-tuning with just a few clicks in the dashboard

### Use Cases
- Reducing costs by optimizing smaller models
- Improving output consistency and format
- Shortening system prompts without sacrificing performance

## Tracing

Tracing provides insights into your application's performance and behavior:

### Features
- **Monitor Execution Flow**: Track function calls and measure durations
- **Inspect Inputs and Outputs**: Analyze how data flows through your system
- **Identify Bottlenecks**: Pinpoint areas for optimization

<Info>
  PromptLayer automatically tracks LLM requests, capturing inputs, outputs, request durations, and metadata. Use traces to debug, optimize, and enhance your applications.
</Info>

## Custom Evaluations

The Evaluations feature allows you to assess prompts using a visual interface:

### Key Features
- **Visual Pipeline Builder**: Create and modify evaluation steps without coding
- **Flexible Steps**: Add prompt templates, manual grading, or string comparison
- **Continuous Improvement**: Analyze results to iteratively enhance your prompts

<Tip>
  Custom evaluations are ideal for bulk jobs, regression testing, and creating tailored scoring criteria.
</Tip>

## Workspaces

Workspaces enable seamless collaboration:

### Features
- **Centralized Organization**: Share requests, templates, and evaluations in a unified workspace
- **Team Collaboration**: Create separate workspaces for different teams or projects
- **Environment-Specific Authentication**: Use unique API keys for each workspace to maintain security

<Info>
  Workspaces simplify project management, making it easy for teams to collaborate and manage prompts efficiently.
</Info>

<Card>
  <h3>ðŸŽ¯ Next Steps</h3>
  Ready to dive deeper? Check out our [detailed guides](/guides) or join our [community](/community) for support and inspiration.
</Card>