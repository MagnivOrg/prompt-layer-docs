---
title: "Create Evaluation Pipeline"
openapi: "POST /reports"
---

This endpoint creates an Evaluation Pipeline associated with a dataset.

## Request Parameters

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `dataset_group_id` | `integer` | Yes | ID of the dataset group to use |
| `name` | `string` | No | Name for the pipeline (auto-generated if not provided) |
| `folder_id` | `integer` | No | Folder ID for organization |
| `dataset_version_number` | `integer` | No | Specific dataset version (uses latest if not provided) |
| `columns` | `array` | No | List of evaluation columns to add |
| `score_configuration` | `object` | No | Custom scoring logic configuration |

## Column Definition

Each column in the `columns` array accepts:

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `column_type` | `string` | Yes | Type of column (see supported types below) |
| `name` | `string` | Yes | Display name for the column |
| `configuration` | `object` | Yes | Column-type-specific configuration |
| `position` | `integer` | No | Position in the pipeline (auto-assigned if not provided) |
| `is_part_of_score` | `boolean` | No | Whether this column contributes to the score (defaults to `false`) |

### Supported Column Types

<Info>
For detailed configuration options for each column type, see the [Node & Column Types](/features/evaluations/column-types) documentation.
</Info>

| Type | Description |
|------|-------------|
| `PROMPT_TEMPLATE` | Runs a prompt template from the registry |
| `CODE_EXECUTION` | Executes custom Python or JavaScript code |
| `ENDPOINT` | Calls an external HTTP endpoint |
| `WORKFLOW` | Runs a PromptLayer workflow |
| `MCP` | Executes an MCP action |
| `HUMAN` | Manual human evaluation |
| `CONVERSATION_SIMULATOR` | Simulates multi-turn conversations for testing chatbots |
| `LLM_ASSERTION` | AI-powered assertion that evaluates content against a prompt |
| `AI_DATA_EXTRACTION` | AI-powered data extraction from content |
| `COMPARE` | Compares two values for equality |
| `CONTAINS` | Checks if a value contains a substring |
| `REGEX` | Matches content against a regular expression |
| `COSINE_SIMILARITY` | Calculates semantic similarity between texts |
| `ABSOLUTE_NUMERIC_DISTANCE` | Calculates absolute difference between numbers |
| `JSON_PATH` | Extracts data using JSONPath expressions |
| `XML_PATH` | Extracts data using XPath expressions |
| `REGEX_EXTRACTION` | Extracts content using a regular expression |
| `PARSE_VALUE` | Parses and converts values to a specific type |
| `VARIABLE` | Static value or constant |
| `ASSERT_VALID` | Validates data format (JSON, number, SQL) |
| `COALESCE` | Returns first non-null value from multiple sources |
| `COMBINE_COLUMNS` | Combines multiple column values into a dict |
| `COUNT` | Counts chars, words, sentences, or paragraphs |
| `MATH_OPERATOR` | Numeric comparison operations |
| `MIN_MAX` | Finds minimum or maximum values |

## Scoring

There are two independent ways to calculate scores:

### Built-in Scoring (`is_part_of_score`)

Set `is_part_of_score: true` on columns to have PromptLayer automatically average their values into a score.

### Custom Scoring (`score_configuration`)

Write custom code that receives all report data and calculates the score however you want.

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `code` | `string` | Yes | Python or JavaScript code for score calculation |
| `code_language` | `string` | No | `"PYTHON"` (default) or `"JAVASCRIPT"` |

Your custom code receives a `data` variable (list of row dictionaries with all column values) and must return a dictionary with at least a `score` key (0-100).

## Examples

### Built-in Scoring

Use `is_part_of_score: true` to have PromptLayer automatically average the column values:

```python
import requests

response = requests.post(
    "https://api.promptlayer.com/reports",
    headers={"X-API-Key": "your_api_key"},
    json={
        "dataset_group_id": 123,
        "name": "Pipeline with Built-in Scoring",
        "columns": [
            {
                "column_type": "LLM_ASSERTION",
                "name": "Accuracy Check",
                "configuration": {
                    "source": "response",
                    "prompt": "Is this response accurate?"
                },
                "is_part_of_score": True
            },
            {
                "column_type": "LLM_ASSERTION",
                "name": "Safety Check",
                "configuration": {
                    "source": "response",
                    "prompt": "Is this response safe?"
                },
                "is_part_of_score": True
            }
        ]
    }
)
```

### Custom Scoring

Use `score_configuration` to write custom scoring logic:

```python
import requests

response = requests.post(
    "https://api.promptlayer.com/reports",
    headers={"X-API-Key": "your_api_key"},
    json={
        "dataset_group_id": 123,
        "name": "Pipeline with Custom Scoring",
        "columns": [
            {
                "column_type": "LLM_ASSERTION",
                "name": "Accuracy Check",
                "configuration": {
                    "source": "response",
                    "prompt": "Is this response accurate?"
                }
            },
            {
                "column_type": "LLM_ASSERTION",
                "name": "Safety Check",
                "configuration": {
                    "source": "response",
                    "prompt": "Is this response safe?"
                }
            }
        ],
        "score_configuration": {
            "code": """
# Weighted scoring: accuracy matters more
weights = {"Accuracy Check": 0.7, "Safety Check": 0.3}
total_weight = weighted_sum = 0

for row in data:
    for col, weight in weights.items():
        if col in row:
            total_weight += weight
            if row[col] == True:
                weighted_sum += weight

score = (weighted_sum / total_weight * 100) if total_weight > 0 else 0
return {"score": round(score, 2)}
""",
            "code_language": "PYTHON"
        }
    }
)
```
