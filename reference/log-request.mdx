---
title: "Log Request"
openapi: "POST /log-request"
---

Log a request to the system. This is useful for logging requests from custom LLM providers.

## Using Structured Outputs

When logging requests that use structured outputs (JSON schemas), include the schema configuration in the `parameters` field using the `response_format.json_schema` structure.

**Example:**

```json
{
  "provider": "openai",
  "model": "gpt-4",
  "api_type": "chat-completions"
  "parameters": {
    "temperature": 0.7,
    "response_format": {
      "type": "json_schema",
      "json_schema": {
        "name": "YourSchemaName",
        "schema": {
          "type": "object",
          "properties": {
            "field1": {"type": "string"}
          },
          "required": ["field1"]
        }
      }
    }
  }
}
```

For complete examples with OpenAI, Anthropic, Google Gemini, and detailed implementation guidance, see:

**[Logging Structured Outputs Guide →](/features/prompt-history/structured-output-logging)**

## Using Extended Thinking / Reasoning

When logging requests that use extended thinking (Anthropic), thinking mode (Google), or reasoning (OpenAI), the configuration must be passed inside the `parameters` field using provider-specific formats:

| Provider | Parameter | Example |
|----------|-----------|---------|
| Anthropic | `thinking` | `{"thinking": {"type": "enabled", "budget_tokens": 10000}}` |
| Google | `thinking_config` | `{"thinking_config": {"include_thoughts": true, "thinking_budget": 8000}}` |
| OpenAI | `reasoning_effort` | `{"reasoning_effort": "high"}` |

For complete examples with thinking content blocks and full code samples, see:

**[Logging Extended Thinking Guide →](/features/prompt-history/custom-logging#logging-extended-thinking-and-reasoning)**

## Error Tracking

You can log failed or problematic requests using the `status`, `error_type`, and `error_message` fields. This is useful for monitoring error rates, debugging issues, and tracking provider reliability.

### Example: Logging a Failed Request

```json
{
  "provider": "openai",
  "model": "gpt-4",
  "api_type": "chat-completions",
  "input": {
    "type": "chat",
    "messages": [{"role": "user", "content": "Hello"}]
  },
  "output": {
    "type": "chat",
    "messages": []
  },
  "request_start_time": "2024-01-15T10:30:00Z",
  "request_end_time": "2024-01-15T10:30:30Z",
  "status": "ERROR",
  "error_type": "PROVIDER_TIMEOUT",
  "error_message": "Request timed out after 30 seconds"
}
```

### Example: Logging a Warning

Use `WARNING` status for requests that succeeded but had issues (e.g., retries, degraded responses):

```json
{
  "provider": "anthropic",
  "model": "claude-3-sonnet",
  "api_type": "chat-completions",
  "input": {
    "type": "chat",
    "messages": [{"role": "user", "content": "Summarize this"}]
  },
  "output": {
    "type": "chat",
    "messages": [{"role": "assistant", "content": "Summary..."}]
  },
  "request_start_time": "2024-01-15T10:30:00Z",
  "request_end_time": "2024-01-15T10:30:05Z",
  "status": "WARNING",
  "error_type": "PROVIDER_RATE_LIMIT",
  "error_message": "Succeeded after 2 retries due to rate limiting"
}
```

## Related Documentation

- [Custom Logging Guide](/features/prompt-history/custom-logging) - General guide to logging requests
- [Structured Outputs in Prompt Registry](/features/prompt-registry/structured-outputs) - Creating prompts with structured outputs
- [Metadata Documentation](/features/prompt-history/metadata) - Using metadata for tracking