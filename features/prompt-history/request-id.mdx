---
title: "Request IDs"
icon: "id-card"
---

Every PromptLayer log has a unique PromptLayer Request ID (`pl_id`). 

All tracking in PromptLayer is based on the `pl_request_id`. This identifier is needed to enrich logs with [metadata](/features/prompt-history/metadata), [scores](/features/prompt-history/scoring-requests), [associated prompt templates](/features/prompt-history/tracking-templates), and more.

You can quickly grab a request ID from the web UI as shown below.

<img src="/images/copy-request-id.gif" width="60%"/>

Specific instructions for retrieving the ID programmatically are below.

## REST API
The `pl_request_id` is returned as `request_id` in the case of a successful request when using the `REST api` with `/track-request`. This means that `request_id` will be a key in the object returned by a successful track-request response.

[Learn more](/reference/track-request)

## Python

To retrieve the `pl_request_id` using OpenAI in Python (through `promptlayer.openai`), set the argument `return_pl_id` to `true` in your function call. This will change the call to now return the OpenAI response and the `pl_request_id`. If you are using `stream=true`, then only the last `pl_request_id` will be the id; otherwise, it will be `None`.

The same is true for Anthropic.
        
```python
from promptlayer import openai
response, pl_request_id = openai.Completion.create(
    engine="text-davinci-003", 
    prompt="hi my name is ", 
    return_pl_id=True
)
```

## LangChain

### With Callback (newer)

For LangChain PromptLayer integration, define a `pl_id_callback` callback inside the `PromptLayerCallbackHandler`. This takes in the `pl_request_id` and can be used accordingly. Here's an example:

```python 
def pl_id_callback(pl_request_id):
    print(pl_request_id)

llm = OpenAI(
    model_name="text-davinci-003",
    callbacks=[
        PromptLayerCallbackHandler(
            pl_id_callback=pl_id_callback, 
            pl_tags=["langchain"]
        )
    ],
)

llm("How tall are you?")
```   

### With Specific Models (older)

For LangChain PromptLayer integration, set the argument `return_pl_id` to true when instantiating your model. This will add the PromptLayer request ID in the `generation_info` field of the Generation returned when using `.generate` or `.agenerate`. Here's an example:
        
```python
from langchain.llms import PromptLayerOpenAI
llm = PromptLayerOpenAI(return_pl_id=True)

llm_results = llm.generate(["hello world"])
for res in llm_results.generations:
    print("pl request id: ", res[0].generation_info["pl_request_id"])
```