---
title: "Prompt Registry"
icon: "notebook"
---

The Prompt Registry allows you to easily manage your prompt templates, which are customizable prompt strings with placeholders for variables.

Specifically, a prompt template is your prompt string with variables indicated in curly brackets (`This is a prompt by {author_name}`). Prompt templates can have tags and are uniquely named.

You can use this tool to programmatically retrieve and publish prompts (even at runtime!). That is, this registry makes it easy to start A/B testing your prompts. Viewed as a "**Prompt Management System**”, this registry allows your org to pull out and organize the prompts that are currently dispersed throughout your codebase.

<video controls="controls">
  <source src="/videos/registry.mp4" type="video/mp4" />
</video>

## Collaboration

The Prompt Registry is perfect for engineering teams looking to organize & track their many prompt templates.

... but the **real** power of the registry is collaboration. Engineering teams waste cycles deploying prompts and content teams are often blocked waiting on these deploys.

By programmatically pulling down prompt templates at runtime, product and content teams can visually update & deploy prompt templates without waiting on eng deploys.

We know quick feedback loops are important, but the Prompt Registry also makes those annoying last-minute prompt updates easy.

## Getting a Template

The Prompt Registry is designed to be used at runtime to pull the latest prompt template for your request.

It’s simple.

<CodeGroup>

```python Python SDK
template_dict = promptlayer.prompts.get("my_template")
```

```js JavaScript
const template_dict = await promptlayer.prompts.get({ prompt_name: "my_template" });
```

</CodeGroup>

Alternatively, use the REST API endpoint `/rest/get-prompt-template` ([read more](/reference/get-prompt-template)).

### By Release Label

Release labels like `prod` and `staging` can be optionally applied to template versions and used to retrieve the template.

<CodeGroup>

```python Python SDK
promptlayer.prompts.get("my_template", label="prod")
```

```js JavaScript
const template_dict = await promptlayer.prompts.get({
  prompt_name: "my_template",
  label: "prod"
});
```

</CodeGroup>

### By Version

You can also optionally pass `version` to get an older version of a prompt. By default the newest version of a prompt is returned

<CodeGroup>

```python Python SDK
template_dict = promptlayer.prompts.get("my_template", version=3)
```

```js JavaScript
const template_dict = await promptlayer.prompts.get({
  prompt_name: "my_template", version: 3
});
```

</CodeGroup>

### Metadata

When fetching a prompt template, you can optionally retrieve metadata by using `include_metadata=True` in Python SDK. This will provide you with a tuple containing both the prompt template and the metadata. For example:

<CodeGroup>

```python Python SDK
template, metadata = promptlayer.prompts.get("my_template", include_metadata=True)
```

```js JavaScript
const template = await promptlayer.prompts.get({
  prompt_name: "my_template"
});
```

</CodeGroup>

Notice you don't need to specify `include_metadata=True` in JavaScript SDK, as metadata is always included.

### With LangChain

Optionally, to return a LangChain `PromptTemplate` type include `langchain=True` as an optional argument as shown below.

```python Python SDK
prompt_template_obj = promptlayer.prompts.get("my_template", langchain=True)
```

## Publishing a Template

You can use the UI or API to create a template.

Templates are unique by name, which means that publishing a template with the same name will overwrite old templates.

### String Formats

You may choose to select from one of the two supported template formats (`f-string, jinja2`) to declare variables. (`f-string`) allows you to declare variables using curly brackets (`{variable_name}`) while (`jinja2`) allows you to declare variables using double curly brackets (`{{variable_name}}`).

<img src="/images/template-format.png" />

### Visually

<img src="/images/prompt-template.png" />

To create a template using the UI, simply navigate to the Registry and click "Create Template". This will allow you to create the template visually. You can also edit old templates from the UI.

Rename a template by triple-clicking on its name. It’s best practice to keep template names unique and lowercase.

### Programmatically

While it's easiest to publish prompt templates visually through the dashboard, some users prefer the programatic interface detailed below.

<CodeGroup>

```python Python SDK
promptlayer.prompts.publish(
    "my_template", prompt_template=my_template, tags=["my_tag"])
```

```js JavaScript
promptlayer.prompts.publish({
  prompt_name: "my_template",
  prompt_template: my_template,
  tags: ["my_tag"],
});
```

</CodeGroup>

### Release Labels

Prompt labels are a way to put a label on your prompt template to help you organize and search for them. This enables you to get a specific version of prompt using the label. You can add as many labels as you want to a prompt template with one restriction - the label must be unique across all versions. This means that you cannot have a label called `prod` on both version 1 and version 2 of a prompt template. This restriction is in place to prevent confusion when searching for prompt templates.

<video controls="controls">
  <source src="/videos/prompt-label.mp4" type="video/mp4" />
</video>

### Commit Messages

Prompt commit messages allow you to set a brief 72 character-long description on each of your prompt template version to help you keep track of changes.

<video controls="controls">
  <source src="/videos/prompt-version-commit-message.mp4" type="video/mp4" />
</video>

You can also retrieve the commit messages through code. You'll see them when you list all templates.

<CodeGroup>

```python Python SDK
promptlayer.prompts.all()
```

```js JavaScript
promptlayer.prompts.all()
```

</CodeGroup>

Or set them, by specifing a commit_message arg to prompts.publish, like this:

<CodeGroup>

```python Python SDK
pt = promptlayer.prompts.get("demo-template")

promptlayer.prompts.publish(
    "demo-template",
    commit_message="This is a test",
    prompt_template=pt,
)
```

```js JavaScript
const pt = await promptlayer.prompts.get({
  prompt_name: "demo-template"
});

promptlayer.prompts.publish({
  prompt_name: "demo-template",
  commit_message: "This is a test",
  prompt_template: pt,
});
```

</CodeGroup>

They are also available through the following REST API endpoints

`/rest/get-prompt-template` ([read more](/reference/get-prompt-template))

`/rest/publish-prompt-template` ([read more](/reference/publish-prompt-template))

### Metadata

Custom metadata can be associated with individual prompt template versions. This allows you to set values such as provider, model, temperature, or any other key-value pair for each prompt template version. Please note that the `model` attribute is reserved for model parameters, avoid putting custom metadata here.

<img src="/images/model-parameters.png" />

Custom non-model related metadata can be seen and edited through the Prompt Template Version edit page.

<img src="/images/template-metadata.png" />

You can use our Python SDK to publish a prompt template with metadata. For example, here's how you can set the `model` metadata along with a custom `category` metadata:

<CodeGroup>

```python Python SDK
promptlayer.prompts.publish(
    "my_template",
    prompt_template=my_template,
    metadata={
        "model": {
            "provider": "openai",
            "name": "gpt-3.5-turbo",
            "parameters": {"temperature": 0.5, "max_tokens": 256},
        },
        "category": "weather",
    }
)
```

```js JavaScript
promptlayer.prompts.publish({
  prompt_name: "my_template",
  prompt_template: my_template,
  metadata: {
    model: {
      provider: "openai",
      name: "gpt-3.5-turbo",
      parameters: { temperature: 0.5, max_tokens: 256 },
    },
    category: "weather",
  },
});
```

</CodeGroup>

Alternatively, use the REST API endpoint `/rest/publish-prompt-template` ([read more](/reference/publish-prompt-template)).

You can also use Langchain to create a template, either by pulling it from LangchainHub, creating a custom template, or providing a Python dictionary directly.

## Tracking templates

The power of the Prompt Registry comes from associating requests with the template they used. This allows you to track average score, latency, and cost of each prompt template. You can also see the individual requests using the template.

![Prompt registry states](/images/getting-started-prompt-registry-stats.png)

To associate a request with a prompt template and some input variables, use the code below with the [pl_request_id](/features/prompt-history/request-id) for the request.

<CodeGroup>

```python Python SDK
promptlayer.track.prompt(request_id=pl_request_id,
    prompt_name='<PROMPT_NAME>', prompt_input_variables={...})
```

```js JavaScript
promptlayer.track.prompt({
  request_id: pl_request_id,
  prompt_name: '<PROMPT_NAME>',
  prompt_input_variables: {...} 
})
```

</CodeGroup>

Alternatively, use the REST API endpoint `/rest/track-prompt` ([read more](/reference/track-prompt)).

Learn more about tracking templates [here](/features/prompt-history/tracking-templates)

## A/B Testing

The Prompt Registry allows you to manage and publish prompt templates programmatically, which makes it easier to separate prompts from the codebase and to perform A/B testing.

CMS systems like the Prompt Registry are useful for A/B testing because they allow you to programmatically manage and publish different versions of a prompt template. By creating multiple versions of a prompt template, you can serve different prompts to different users and measure which version performs better. This can help you optimize your prompts and improve user engagement.

## OpenAI Functions

The Prompt Registry supports [OpenAI functions](https://openai.com/blog/function-calling-and-other-api-updates) format. You can create a prompt using functions both visually and programmatically.

Be sure to read [this OpenAI guide](https://platform.openai.com/docs/guides/gpt/function-calling) for more context.

### Publishing Visually

Functions can be defined, called, and set up visually through the Prompt Registry.

<img src="/images/create-function.gif" />

### Publishing Programmatically

To publish a prompt template with functions programatically is very similar to how you would publish a regular prompt template. To add functions you can add the arguments `functions` and `function_call` to your `prompt_template` object.

If you’re already calling OpenAI API, you can bundle the functions and publish it into the Prompt Registry, like this:

<CodeGroup>

```python Python SDK
messages = [{"role": "user", "content": "What's the weather like in Boston?"}]
functions = [
  {
    "name": "get_current_weather",
    "description": "Get the current weather in a given location",
    "parameters": {
      "type": "object",
      "properties": {
        "location": {
          "type": "string",
          "description": "The city and state, e.g. San Francisco, CA",
        },
        "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
      },
      "required": ["location"],
    },
  }
]

promptlayer.prompts.publish(
  prompt_name="test_chat_functions",
  prompt_template={
    "function_call": "auto", # or {"name": function_name} if you want to make sure the function gets called.
    "input_variables": [], # list them here if it has any
    "messages": [
      {
        "role": role,
        "prompt": {
          "input_variables": [], # list them here if it has any
          "template": content,
          "template_format": "f-string", # or "jinja"
          "validate_template": True, # optional
          "_type": "prompt",
        },
      } for (role, content) in map(lambda x: x.values(), messages)
    ],
    "functions": functions,
    "_type": "chat_promptlayer_langchain", # important if using gpt-3.5-turbo or gpt-4
  },
)

response = openai.ChatCompletion.create(
  model="gpt-3.5-turbo",
  messages=messages,
  functions=functions,
  function_call="auto",
)
```

```js JavaScript
const messages = [
  { role: "user", content: "What's the weather like in Boston?" },
];
const functions = [
  {
    name: "get_current_weather",
    description: "Get the current weather in a given location",
    parameters: {
      type: "object",
      properties: {
        location: {
          type: "string",
          description: "The city and state, e.g. San Francisco, CA",
        },
        unit: { type: "string", enum: ["celsius", "fahrenheit"] },
      },
      required: ["location"],
    },
  },
];

promptlayer.prompts.publish({
  prompt_name: "test_chat_functions",
  prompt_template: {
    function_call: "auto", // or {"name": function_name} if you want to make sure the function gets called.
    input_variables: [], // list them here if it has any
    messages: messages.map(({ role, content }) => ({
      role,
      prompt: {
        input_variables: [], // list them here if it has any
        template: content,
        template_format: "f-string", // or "jinja"
        validate_template: true, // optional
        _type: "prompt",
      },
    })),
    functions,
    _type: "chat_promptlayer_langchain", // important if using gpt-3.5-turbo or gpt-4
  },
});

const response = await openai.completions.create({
  model: "gpt-3.5-turbo",
  messages,
  functions,
  function_call: "auto",
});
```

</CodeGroup>

### Getting a Prompt Programmatically

To get a prompt template that uses OpenAI functions programatically you will need to parse the template from the Prompt Registry format to what OpenAI expects.

The following code snippet shows you how to do this:

<CodeGroup>

```python Python SDK
def get_message(chat_message):
    if "prompt" in chat_message:
        return {"role": chat_message.get("role"), "content":chat_message.get("prompt", {}).get("template", "")}
    return chat_message

prompt = promptlayer.prompts.get(
    prompt_name="my_prompt_with_functions"
)
messages = [get_message(message) for message in prompt.get("messages", [])]
functions = prompt.get("functions", [])
function_call = prompt.get("function_call")

response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=messages,
    functions=functions,
    function_call=function_call
)
```

```js JavaScript
const get_message = (chat_message) => {
  if ("prompt" in chat_message) {
    return {
      role: chat_message.role,
      content: chat_message.prompt.template,
    };
  }
  return chat_message;
};

const prompt = await promptlayer.prompts.get({
  prompt_name: "my_prompt_with_functions",
});

const messages = prompt.messages.map(get_message);
const functions = prompt.functions;
const function_call = prompt.function_call;

const response = await openai.completions.create({
  model: "gpt-3.5-turbo",
  messages,
  functions,
  function_call,
});
```

</CodeGroup>

## Getting All Prompts Programmatically

To get all prompts from prompt registry you can use the following code snippet:

<CodeGroup>

```python Python SDK
from promptlayer import prompts

all_prompts = prompts.all()
```

```js JavaScript
const all_prompts = promptlayer.prompts.all()
```

</CodeGroup>

By default this returns 30 prompts. You can change this by passing in the `per_page` argument. For example, to get 100 prompts you can do the following:

<CodeGroup>

```python Python SDK
all_prompts = prompts.all(per_page=100)
```

```js JavaScript
const all_prompts = promptlayer.prompts.all({ per_page: 100 })
```

</CodeGroup>

You can also define the page you want to get by passing in the `page` argument. For example, to get the second page of prompts you can do the following:

<CodeGroup>

```python Python SDK
all_prompts = prompts.all(page=2)
```

```js JavaScript
const all_prompts = promptlayer.prompts.all({ page: 2 })
```

</CodeGroup>

It is important to note that `prompt_template` represents the latest version of the prompt template.

Alternatively, use the REST API endpoint `/rest/prompts` ([read more](/reference/get-prompts)).
