---
title: "Programmatic Evals"
icon: "question"
---

The new Evaluations page is a versatile platform designed to meet the diverse needs of prompt evaluation in production environments. Inspired by the flexibility of tools like Microsoft Excel, this page offers a visual pipeline builder that allows users to construct complex evaluation workflows tailored to their specific requirements. Whether you're scoring prompts, running bulk jobs, or conducting regression testing, the Evaluations page provides the tools needed to assess prompt quality effectively.

[something about nontechnical]

## Key Features

- **Visual Pipeline Builder**: Design and execute evaluation pipelines, whether they are chains or one-off batches. We believe evaluation engineering is half the challenge of getting your prompt right.
- **Continuous Integration for Prompt Templates**: Automatically trigger evaluations with new prompt versions, providing immediate feedback on performance changes. Use historical request data to see how the new version changes output on previous runs.
- **Extensible for Various Applications**: Whether it's for chatbots, RAG systems, or SQL bots, the Evaluations will meet you and your AI or Human evaluators.
- **Flexible Evaluation Strategies**: Support for a wide range of evaluation methods including scoring prompts with golden datasets, bulk job processing, and regression testing.


## Use Cases

1. **Scoring Prompts**
- Utilize golden datasets to compare prompt outputs against ground truths.
- Incorporate human or AI evaluators to assess prompt quality.

2. **One-off Batch Jobs**
- Ideal for experimenting and iterating on prompts, the Evaluations page excels in processing large volumes of input data efficiently.

3. **Regression Testing**
- Connect evaluation pipelines to prompt templates for automatic testing with each new version, using historical request data to identify improvements or regressions.

## Examples of Applications

- **Chatbot Enhancements**: Improve chatbot interactions by evaluating responses to user requests against semantic criteria.
- **RAG System Testing**: Build a RAG pipeline and validate responses against a golden dataset.
- **SQL Bot Optimization**: Test Natural Language to SQL generation prompts by _actually_ running generated queries against a database (using the API Endpoint column), followed by an evaluation of the results' accuracy.

## Getting Started

1. **Select Your Datasets**: Choose or upload datasets to serve as the basis for your evaluations, whether for scoring, regression testing, or bulk job processing.
2. **Build Your Pipeline**: Start by visually constructing your evaluation pipeline, defining each step from input data processing to final evaluation.
3. **Run Evaluations**: Execute your pipeline, observe the results in a spreadsheet-like interface, and make informed decisions based on comprehensive metrics and scores.

### Creating a Batch Run

1. **Initiate a Batch Run**: Start by creating a new batch run, which requires specifying a name and selecting a dataset.
2. **Dataset Selection**: Choose from existing dataset formats (JSON or CSV), or create a dataset from historical data using filters like time range, prompt template logs, scores, and metadata.

#### Preview Mode

- Enter preview mode after dataset selection to iterate with live feedback, allowing for adjustments in real-time.

### Building Your Evaluation Pipeline

#### Adding Columns

- **Add Columns**: Click 'Add Column' to start building your pipeline, with each column representing a step in the evaluation process.

#### Column Types

- **Prompt Template**: Select a prompt template from the registry, set model parameters, LLM, arguments, and template version.
- **Custom API Endpoint**: Define a URL to send and receive data, suitable for custom evaluators or external systems.
- **Human Input**: Engage human graders by adding a column that allows for textual input.
- **String Comparison**: Employ this column to compare the outputs of two different columns, highlighting variances.

### Executing Full Batch Runs

- Transition from preview to full batch run to apply your pipeline across the entire dataset for comprehensive evaluation.

#### Scoring

If the last column (or step) of your evaluation pipeline contains all booleans or numeric values, that will be consider the score for the row. Your full evaluation report will have a scorecard of the average of this last column.

_NOTE: All cells in the last column must be boolean or all must be numeric. If any cell deviates, the score will not be calculated_

## Setting up Continuous Integration

Continuous integration in the context of prompt evaluations involves automatically running evaluations whenever there's a change or update to prompt templates. This ensures that new versions are rigorously tested against historical data, maintaining high standards of prompt performance and functionality. Here's how to set it up:

To facilitate Continuous Integration, the Evaluations page allows you to link prompt templates with evaluations, automatically running tests on new versions using datasets created from historical data. This integration empowers non-technical team members to monitor prompt performance through the dashboard, enabling them to easily interpret results and make informed decisions without delving into complex technical details.

BACKTESTING
