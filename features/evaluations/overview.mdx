---
title: "Evals Overview"
icon: "question"
---

The new Evaluations page is a versatile platform designed to meet the diverse needs of prompt evaluation in production environments. Inspired by the flexibility of tools like Microsoft Excel, this page offers a visual pipeline builder that allows users to construct complex evaluation workflows tailored to their specific requirements. Whether you're scoring prompts, running bulk jobs, or conducting regression testing, the Evaluations page provides the tools needed to assess prompt quality effectively.

[something about nontechnical]

## Key Features

- **Visual Pipeline Builder**: Design and execute evaluation pipelines, whether they are chains or one-off batches. We believe evaluation engineering is half the challenge of getting your prompt right.
- **Continuous Integration for Prompt Templates**: Automatically trigger evaluations with new prompt versions, providing immediate feedback on performance changes. Use historical request data to see how the new version changes output on previous runs.
- **Extensible for Various Applications**: Whether it's for chatbots, RAG systems, or SQL bots, the Evaluations will meet you and your AI or Human evaluators.
- **Flexible Evaluation Strategies**: Support for a wide range of evaluation methods including scoring prompts with golden datasets, bulk job processing, and regression testing.


## Use Cases

1. **Scoring Prompts**
- Utilize golden datasets to compare prompt outputs against ground truths.
- Incorporate human or AI evaluators to assess prompt quality.

2. **One-off Batch Jobs**
- Ideal for experimenting and iterating on prompts, the Evaluations page excels in processing large volumes of input data efficiently.

3. **Regression Testing**
- Connect evaluation pipelines to prompt templates for automatic testing with each new version, using historical request data to identify improvements or regressions.

## Examples of Applications

- **Chatbot Enhancements**: Improve chatbot interactions by evaluating responses to user requests against semantic criteria.
- **RAG System Testing**: Build a RAG pipeline and validate responses against a golden dataset.
- **SQL Bot Optimization**: Test Natural Language to SQL generation prompts by _actually_ running generated queries against a database (using the API Endpoint column), followed by an evaluation of the results' accuracy.