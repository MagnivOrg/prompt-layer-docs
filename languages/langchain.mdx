---
title: 'LangChain'
icon: 'crow'
---

PromptLayer works seamlessly with [LangChain](https://langchain.readthedocs.io). LangChain is a popular Python library aimed at assisting in the development of LLM applications. It provides a lot of helpful features like chains, agents, and memory.

Using PromptLayer with LangChain is simple. See the LangChain docs below:

- [Python Docs](https://python.langchain.com/docs/modules/model_io/models/llms/integrations/promptlayer_openai)

- [Javascript Docs](https://js.langchain.com/docs/modules/models/llms/integrations#promptlayeropenai)

There are two main ways to use LangChain with PromptLayer. The recommended way (as of LangChain version `0.0.221`) is to use the `PromptLayerCallbackHandler`. Alternatively, you can use a PromptLayer-specific LLM or Chat model.

## Using Callbacks

<Info>Right now, callbacks only work with LangChain in Python.</Info>

This is the recommended way to use LangChain with PromptLayer. It is simpler and more extendible than the other method below. 

_Every_ LLM supported by LangChain works with PromptLayer's callback.

Start by importing `PromptLayerCallbackHandler`. This callback function will log your request after each LLM response.

```python
import promptlayer # Don't forget this üç∞
from langchain.callbacks import PromptLayerCallbackHandler
```

Now, when instantiating a model, just include the `PromptLayerCallbackHandler` in the callbacks.

```python
llm = OpenAI(
    model_name="gpt-3.5-turbo-instruct",
    callbacks=[PromptLayerCallbackHandler(pl_tags=["langchain"])],
)
```
üéâ That's it! üéâ

### Full Examples

Below are some full examples using PromptLayer with various LLMs through LangChain.

#### OpenAI

<CodeGroup>

```python Completion
import promptlayer # Don't forget this üç∞
from langchain.callbacks import PromptLayerCallbackHandler

# OpenAI Completion Model
from langchain.llms import OpenAI

llm = OpenAI(
    model_name="gpt-3.5-turbo-instruct",
    callbacks=[
        PromptLayerCallbackHandler(pl_tags=["langchain"])
    ],
)
llm("How tall are you?")
```


```python Chat
import promptlayer # Don't forget this üç∞
from langchain.callbacks import PromptLayerCallbackHandler

# OpenAI Chat Model
from langchain.chat_models import ChatOpenAI
from langchain.schema import (
    AIMessage,
    HumanMessage,
    SystemMessage,
)

chat_llm = ChatOpenAI(
    temperature=0,
    streaming=True,
    callbacks=[PromptLayerCallbackHandler(
        pl_tags=["langchain"]
    )],
)
llm_results = chat_llm(
    [
        SystemMessage(content="You are a funny AI comedian."),
        HumanMessage(content="What comes after 1,2,3 ?"),
    ]
)
print(llm_results)
```

</CodeGroup>

#### GPT4All

```python
import promptlayer # Don't forget this üç∞
from langchain.callbacks import PromptLayerCallbackHandler

from langchain.llms import GPT4All
model = GPT4All(model="./models/gpt4all-model.bin", n_ctx=512, n_threads=8)

response = model("Once upon a time, ", callbacks=[
    PromptLayerCallbackHandler(pl_tags=["langchain", "gpt4all"])])
```

#### HuggingFace Hub

You can use the HuggingFace wrapper to try out many different LLMs.

```python
import promptlayer # Don't forget this üç∞
from langchain.callbacks import PromptLayerCallbackHandler

from langchain import HuggingFaceHub

falcon_repo_id = "tiiuae/falcon-7b-instruct"

llm = HuggingFaceHub(repo_id=falcon_repo_id, 
        huggingfacehub_api_token="<HUGGINFACEHUB_API_TOKEN>", 
        model_kwargs={"temperature": 1.0, "max_length": 64}, 
        callbacks=[PromptLayerCallbackHandler(pl_tags=["langchain", "huggingface"])])

llm("How do you make a layer cake?")
````

#### Async Requests

```python
import promptlayer # Don't forget this üç∞
from langchain.callbacks import PromptLayerCallbackHandler

# OpenAI Completion Model
from langchain.llms import OpenAI

import asyncio
async def async_generate(llm):
    resp = await llm.agenerate(['My name is "'])
    print(resp, pl_request_id)

asyncio.run(async_generate(openai_llm))
```


### PromptLayer Request ID

The [PromptLayer request ID](/features/prompt-history/request-id) is used to tag requests with [metadata](/features/prompt-history/metadata), [scores](/features/prompt-history/scoring-requests), [associated prompt templates](/features/prompt-history/tracking-templates), and more.

`PromptLayerCallbackHandler` can optionally take in its own callback function that takes the request ID as an argument.

<CodeGroup> 

```python Simple
def pl_id_callback(pl_request_id):
    print(pl_request_id)

llm = OpenAI(
    model_name="gpt-3.5-turbo-instruct",
    callbacks=[
        PromptLayerCallbackHandler(
            pl_id_callback=pl_id_callback, 
            pl_tags=["langchain"]
        )
    ],
)

llm("How tall are you?")
```

```python Full Example
# Import required libraries
from langchain_openai import ChatOpenAI
from langchain.callbacks import PromptLayerCallbackHandler
from promptlayer import PromptLayer

# Initialize PromptLayer client
pl_client = PromptLayer()

# Global variables
PROMPT_NAME = ''
INPUT_VARIABLES = {}

# Helper functions
def convert_role(role):
    """Convert 'user' role to 'human', keep others as is."""
    return "human" if role == "user" else role

def convert_content(content):
    """Extract text content from list or return as is."""
    return content[0]["text"] if isinstance(content, list) else content

def track_prompt_usage(prompt_layer_id):
    """Track prompt usage in PromptLayer."""
    pl_client.track.prompt(prompt_name=PROMPT_NAME, request_id=prompt_layer_id, prompt_input_variables=INPUT_VARIABLES)

# Initialize LangChain ChatOpenAI with PromptLayer callback
llm = ChatOpenAI(
    model="gpt-4o",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    callbacks=[PromptLayerCallbackHandler(pl_id_callback=track_prompt_usage)]
)

PROMPT_NAME = "ai-poet-test"
INPUT_VARIABLES = {"topic": "PromptLayer the Premier Prompt Engineering Platform"}
template = pl_client.templates.get(PROMPT_NAME, {"input_variables": INPUT_VARIABLES})
pl_messages = template["llm_kwargs"]["messages"]

messages = [
    (convert_role(pl_message["role"]), convert_content(pl_message["content"])) for pl_message in pl_messages
]
ai_msg = llm.invoke(messages)

PROMPT_NAME = "check-the-weather"
INPUT_VARIABLES = {'location': 'New York'}
template = pl_client.templates.get(PROMPT_NAME, {"input_variables": INPUT_VARIABLES})
pl_messages = template["llm_kwargs"]["messages"]
messages = [
    (convert_role(pl_message["role"]), convert_content(pl_message["content"])) for pl_message in pl_messages
]
ai_msg = llm.invoke(messages)
```

</CodeGroup>


## PromptLayer OpenAI Models

Alternatively, the older (but still supported) way to use LangChain with PromptLayer is through specific PromptLayer LLMs and Chat Models.

Please note: Do not use these models in addition to the callback. Use one or the other.

See below for examples:

<CodeGroup>

 ```python Chat (Recommended)
from langchain.chat_models import PromptLayerChatOpenAI
from langchain.schema import (
    SystemMessage,
    HumanMessage,
)
chat = PromptLayerChatOpenAI(pl_tags=["langchain"])
chat([
  SystemMessage(content="You are a helpful assistant that translates English to French."),
  HumanMessage(content="Translate this sentence from English to French. I love programming.")
])
```

```python Chat (Alternate)
from langchain.llms import PromptLayerOpenAIChat
llm = PromptLayerOpenAIChat()
resp = llm("tell me a joke")
```

```python Completion
from langchain.llms import PromptLayerOpenAI
llm = PromptLayerOpenAI(pl_tags=["langchain"])
llm("My name is")
```

</CodeGroup>
    