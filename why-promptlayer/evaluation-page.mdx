---
title: "Evaluations"
icon: "question"
---

The Evaluations page allows users to run a prompt against a batch of test cases from a JSON or CSV file. This documentation will guide you through the various features and options available on the Evaluations page.

The Evaluations page provides a powerful tool for testing prompts, allowing PMs and non-technical team members to easily evaluate prompt performance. It is particularly useful for several purposes:

- **Testing and Validation**: Evaluations enable thorough testing of prompts by running them against a batch of test cases from JSON or CSV files. This feature is helpful in identifying edge cases, ensuring robustness, and validating prompt behavior.

- **Quality Comparison**: Evaluations facilitate comparing prompts to understand their relative quality and effectiveness. By running multiple prompt templates on the same set of test cases, you can evaluate and compare their outputs to make informed decisions.

- **Regression Prevention**: Evaluations help prevent regression by testing prompts against real usage data. By running evaluations periodically, you can detect any deviations in prompt behavior, address potential issues, and ensure consistent performance.

![Evaluations Page](/images/evaluations-page.gif)

## Running Evaluations

The Evaluations page allows users to run a prompt template from the [Prompt Registry](/features/prompt-registry) with custom hyperparameters. Users can also select from multiple models to run the prompt template on, including models from OpenAI, Anthropic, and more (additional models coming soon). We also provide the option to add delays between requests to avoid rate limits.

The evaluation run output provides various statistics, including latency, cost, average latency, average cost, and distributions. Each request can be viewed as a normal PromptLayer request, and retries are available if needed. The output can be shared with your team and exported as a CSV file for further analysis.

## Test Cases File Formats

JSON or CSV files are accepted for the test cases file. These files contain the test cases that will be used as input variables for the prompt template. Here's an explanation of each format:

### JSON Format

In the JSON format, each test case is represented as a separate JSON object. The keys of the object correspond to the input variable names defined in the prompt template. The values represent the specific input values for each test case. Here's an example:

```json
[
  {
    "name": "John Doe",
    "age": 30,
    "location": "New York"
  },
  {
    "name": "Jane Smith",
    "age": 35,
    "location": "Los Angeles"
  },
  {
    "name": "Michael Johnson",
    "age": 40,
    "location": "Chicago"
  }
]

```

In the above example, the prompt template may contain input variables like `{name}`, `{age}`, and `{location}`. Each test case object provides the corresponding values for these variables.

### CSV Format

In the CSV format, each test case is represented as a separate row in the CSV file. The column headers correspond to the input variable names defined in the prompt template. The cells in each row represent the specific input values for each test case. Here's an example:

```
name,age,location
John Doe,30,New York
Jane Smith,35,Los Angeles
Michael Johnson,40,Chicago
```

In this example, the prompt template may contain input variables like `{name}`, `{age}`, and `{location}`. Each row in the CSV file provides the corresponding values for these variables.

## Output Parsing

The Evaluations page also offers an "Output Parsing" feature that allows you to extract variables from the prompt response and place them into different columns. This feature provides several parsing options:

- **None**: No output parsing is applied.
- **Parse Tags**: The output is parsed based on tags present in the response string. You can enter the tags as comma-separated values.
- **Custom Parser**: Contact us if you would like to add a custom parser.

Please note that for the "Parse Tags" option to work correctly, it is the responsibility of the prompt engineers to ensure that the tags appear in the prompt output in the expected format. The assumption is that the tags are present in the string in order, each followed by a colon. Tag keys are parsed exactly as they appear without modifications, and no tag will occur twice in the completion string. The function iterates through the tags, splitting the string using each tag as a delimiter. Each tag corresponds to a column name.

**Example Input:**

Tags: `["mode", "first", "last_three"]`

Output #1: `ASSISTANT: mode: Alice first: In last_three: the prestigious award.`

![Before Output Parsing Screenshot](/images/before-output-parse.png)

**Example Output:**

Output #1:
- mode -> `Alice`
- first -> `In`
- last_three -> `the prestigious award.`

![After Output Parsing Screenshot](/images/after-output-parse.png)