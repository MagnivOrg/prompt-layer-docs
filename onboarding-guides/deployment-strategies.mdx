---
title: "Deployment Strategies"
description: Choose from three integration patterns. Direct SDK calls, webhook caching, or managed agents.
icon: ship
---

PromptLayer fits into your stack at three levels of sophistication:

1. **`promptlayer_client.run`** ‚Äì zero-setup SDK sugar
2. **Webhook-driven caching** ‚Äì maintain local cache of prompt templates
3. **Managed Agents** ‚Äì let PromptLayer orchestrate everything server-side


<Frame>
![PromptLayer Deployment Strategies](/onboarding-guides/images/deployment%20strategies.png)
</Frame>

---

# Use `promptlayer_client.run` (quickest path)

When every millisecond of developer time counts, call `promptlayer_client.run()` directly from your application code.
1. Fetch latest prompt ‚Äì We pull the template (by version or release label) from PromptLayer.
2. Execute ‚Äì The SDK sends the populated prompt to OpenAI, Anthropic, Gemini, etc.
3. Log ‚Äì The raw request/response pair is saved back to PromptLayer.

<CodeGroup>

```python Python
from promptlayer import PromptLayer
pl_client = PromptLayer(api_key="...")

response = pl_client.run(
    prompt_name="order-summary",
    input_variables={"cart": cart_items},
    prompt_release_label="prod"
)
````

```js JavaScript
import { PromptLayer } from "promptlayer";
const plClient = new PromptLayer({ apiKey: "..." });

const response = await plClient.run({
  promptName: "order-summary",
  inputVariables: { cart: cartItems },
  promptReleaseLabel: "prod"
});
```

</CodeGroup>

**Under the hood**

1. SDK pulls the latest prompt (or the version/label you specify).
2. Your client calls the model provider (OpenAI, Anthropic, Gemini, ‚Ä¶).
3. SDK writes the log back to PromptLayer.

> üí° **Tip** ‚Äì If latency is critical, enqueue the log to a background worker and let your request return immediately.
---

# Cache prompts with Webhooks

Eliminate the extra round‚Äëtrip by **replicating prompts into your own cache or database**.

PromptLayer keeps that cache fresh through webhook events‚Äîno polling required.

```mermaid
sequenceDiagram
  autonumber
  participant PL as PromptLayer Server
  participant APP as Your Application
  participant DB as Your Cache / DB
  participant LLM as Model Provider

  PL->>APP: "prompt.updated" webhook
  APP->>DB: invalidate + store latest prompt
  user->>APP: request needing AI
  APP->>DB: fetch prompt
  APP->>LLM: run prompt
  APP-->>PL: async track.log (optional queue)
```


### Step‚Äëby‚Äëstep

1. **Subscribe to webhooks in the UI**

Read more here about webhooks [here](/onboarding-guides/webhooks).

2. **Maintain a local cache**

```python
# pseudocode
def handle_pl_webhook(event):
    prompt = event["data"]
    db.prompts.upsert(prompt["prompt_template_name"], prompt)
```

3. **Serve traffic**

```python
prompt = db.prompts.get("order-summary")
llm_response = openai.chat.completions.create(...)
queue.enqueue(track_to_promptlayer, llm_response)
```

> **Tip:** Most teams push the track_to_promptlayer onto a Redis or SQS queue so as to not block on the logging of a request.

Read the full guide: **[PromptLayer Webhooks ‚Üó](/onboarding-guides/webhooks)**

---

# Run fully-managed Agents

For complex workflows requiring orchestration, use PromptLayer's managed agent infrastructure.

### How it works
1. Define multi-step workflows in PromptLayer's Agent Builder
2. Trigger agent execution via API
3. Monitor execution on PromptLayer servers
4. Receive results via webhook or polling

PromptLayer handles all orchestration, parallelization, and model provider communication.


### Implementation

<CodeGroup>

```python Python
from promptlayer import PromptLayer
promptlayer_client = PromptLayer(api_key="‚Ä¶")

execution = promptlayer_client.run_workflow(  # SDK method
    workflow_name="customer_support_agent",   # <- this is an Agent
    workflow_label_name="prod",
    input_variables={"ticket_id": 123}
)
```

```ts JavaScript
import { PromptLayer } from "promptlayer";
const promptlayer_client = new PromptLayer({ apiKey: "‚Ä¶" });

const execution = await promptlayer_client.runWorkflow({
  workflowName: "customer_support_agent",    // <- this is an Agent
  workflowLabelName: "prod",
  inputVariables: { ticket_id: 123 }
});

```
</CodeGroup>

Because execution is server-side, you inherit centralized tracing, cost analytics, and secure sandboxed tool-nodes without extra ops.

Learn more: **[Agents documentation ‚Üó](/why-promptlayer/agents)**

---

## Which pattern should I pick?

| Requirement                 | `promptlayer_client.run` | Webhook Cache | Managed Agent |
| --------------------------- | :----------------------: | :-----------: | :-----------: |
| ‚è±Ô∏è *extreme latency reqs*   |             ‚ùå            |       ‚úÖ       |       ‚úÖ       |
| üõ† *Single LLM call*        |             ‚úÖ            |       ‚úÖ       |       ‚ûñ       |
| üå© *Complex plans / tools*  |             ‚ûñ            |       ‚ûñ       |       ‚úÖ       |
| üë• *Non-eng prompt editors* |             ‚úÖ            |       ‚úÖ       |       ‚úÖ       |
| üß∞ *Zero ops overhead*      |             ‚úÖ            |       ‚ûñ       |       ‚úÖ       |

---

## Further reading üìö

* **Quickstart** ‚Äì [Your first prompt](/quickstart)
* **Webhooks** ‚Äì [Events & signature verification](/onboarding-guides/webhooks)
* **Zero Downtime Deploys** ‚Äì [Deploy new variables safely](/onboarding-guides/zero-downtime-deploys)
* **Agents** ‚Äì [Concepts & versioning](/why-promptlayer/agents)
* **Evaluation** ‚Äì [Building evaluation pipelines](/features/evaluations/getting-started)

---

> ‚úâÔ∏è **Need a hand?** Ping us in Discord or email [hello@promptlayer.com](mailto:hello@promptlayer.com)‚Äîhappy to chat architecture!