---
title: "Evaluation"
description: "Evaluate your prompts to measure quality and effectiveness through systematic testing."
hidden: true
---

**Evaluation** enables you to measure prompt quality and effectiveness through systematic testing. Below are the key steps to import or create test datasets, configure evaluation metrics, and run evaluationsâ€”with clear instructions for each stage.

## Import or Create Test Datasets

Establish a dataset by importing existing data or creating a new one. This provides the necessary context for comprehensive prompt testing.

**Step by Step:**

1. Click **Datasets** in the left sidebar.
2. (Optional) Download example datasets.
3. Click the **Create Dataset** button.
   1. If you have a .csv file, you can upload it.
   2. Otherwise, click on **Add Column**.
   3. Add rows if needed by clicking on the **Add Row** button.
   4. You can add data from your request history by clicking on **Add from Request History**.
4. Click the **Save as New Version** button.

---

## Configure Evaluation Metrics and Scoring Criteria

Set clear metrics and scoring logic to assess prompt performance. This allows you to pinpoint strengths and areas for improvement.

**Step by Step:**

1. Click the **Evaluate** button.
2. Click the **New Batch Run** button.
3. Select a previously created dataset and enter a name.
4. Click **Create Pipeline**.
5. Add steps to evaluate your prompt, set metrics, and scoring criteria.

---

## Run Evaluation

Execute the evaluation process to generate actionable insights. Running evaluations helps compare outputs against expected results and refine your prompts accordingly.

**Step by Step:**

1. Click **Run Full Batch** to start the evaluation.
2. Examine the results:
   1. **Define scoring criteria** to objectively assess performance.
   2. **Compare results** across multiple runs to identify trends.
3. Use these insights to **optimize** and **refine** your prompt.
